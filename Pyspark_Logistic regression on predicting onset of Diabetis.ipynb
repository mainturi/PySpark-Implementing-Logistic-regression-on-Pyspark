{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Logistic Regression using Pyspark to predicit onset of Diabetis. \n",
    "The URL for the Dataset: https://www.kaggle.com/wanglaiqi/pimaindiansdiabetesdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Pregnancies=6, Glucose=148, BloodPressure=72, SkinThickness=35, Insulin=0, BMI=33.6, DiabetesPedigreeFunction=0.627, Age=50, Outcome=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiating a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkSession.builder.appName('Log_Reg').getOrCreate()\n",
    "#Load the Dataset\n",
    "df = spark.read.csv('diabetes.csv', header=True, inferSchema=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
      "|summary|       Pregnancies|          Glucose|     BloodPressure|     SkinThickness|           Insulin|               BMI|DiabetesPedigreeFunction|               Age|           Outcome|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
      "|  count|               768|              768|               768|               768|               768|               768|                     768|               768|               768|\n",
      "|   mean|3.8450520833333335|     120.89453125|       69.10546875|20.536458333333332| 79.79947916666667|31.992578124999977|      0.4718763020833327|33.240885416666664|0.3489583333333333|\n",
      "| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|15.952217567727642|115.24400235133803| 7.884160320375441|       0.331328595012775|11.760231540678689| 0.476951377242799|\n",
      "|    min|                 0|                0|                 0|                 0|                 0|               0.0|                   0.078|                21|                 0|\n",
      "|    max|                17|              199|               122|                99|               846|              67.1|                    2.42|                81|                 1|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: integer (nullable = true)\n",
      " |-- Glucose: integer (nullable = true)\n",
      " |-- BloodPressure: integer (nullable = true)\n",
      " |-- SkinThickness: integer (nullable = true)\n",
      " |-- Insulin: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Outcome: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin|BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+-------+\n",
      "|          0|      0|            0|            0|      0|  0|                       0|  0|      0|\n",
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Finding Nulls in Data\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all from `sql.types`\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['Pregnancies',\n",
    " 'Glucose',\n",
    " 'BloodPressure',\n",
    " 'SkinThickness',\n",
    " 'Insulin',\n",
    " 'BMI',\n",
    " 'DiabetesPedigreeFunction',\n",
    " 'Age']\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=['Pregnancies',\n",
    " 'Glucose',\n",
    " 'BloodPressure',\n",
    " 'SkinThickness',\n",
    " 'Insulin',\n",
    " 'BMI',\n",
    " 'DiabetesPedigreeFunction',\n",
    " 'Age'], outputCol=\"features\")\n",
    "final_data = vec_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: float (nullable = true)\n",
      " |-- Glucose: float (nullable = true)\n",
      " |-- BloodPressure: float (nullable = true)\n",
      " |-- SkinThickness: float (nullable = true)\n",
      " |-- Insulin: float (nullable = true)\n",
      " |-- BMI: float (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: float (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- Outcome: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the Final Data Schema\n",
    "\n",
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = final_data.select('features', 'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "training, test = output_data.randomSplit([.6, .4])\n",
    "test_data = test.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression(labelCol = 'Outcome', featuresCol = 'features')\n",
    "lrmodel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = lrmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7453580901856763"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Outcome')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(training)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7386162687886826"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
